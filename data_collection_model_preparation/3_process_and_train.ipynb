{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import cv2\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras import models, layers, optimizers\n",
    "from keras.applications import VGG16\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pdb\n",
    "\n",
    "% matplotlib inline\n",
    "style.use('seaborn-whitegrid')\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(path):\n",
    "    img = Image.open(path)\n",
    "    img = np.array(img)\n",
    "    return img\n",
    "\n",
    "def process_data(X_data, y_data):\n",
    "    X_data = np.array(X_data, dtype = 'float32')\n",
    "    X_data = np.stack((X_data,)*3, axis=-1)\n",
    "    X_data /= 255\n",
    "    y_data = np.array(y_data)\n",
    "    return X_data, y_data\n",
    "\n",
    "def walk_file_tree(relative_paths, cat_num, max_count):\n",
    "    count_per_dir_dict = {}\n",
    "    num_dirs_in_excess = 0\n",
    "    count_remaining = max_count\n",
    "    X_data = []\n",
    "    for path in relative_paths:\n",
    "        for directory, subdirectories, files in os.walk(path):\n",
    "            num_files = len(files)\n",
    "        count_per_dir_dict[path] = num_files\n",
    "        if num_files <= max_count / len(dirs):\n",
    "            count_remaining -= len(files)\n",
    "        else:\n",
    "            num_dirs_in_excess += 1\n",
    "    for path in relative_paths:\n",
    "        X_data_temp = []\n",
    "        for directory, subdirectories, files in os.walk(path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(directory, file)\n",
    "                X_data_temp.append(process_image(file_path))\n",
    "        # if appropriate amount, process X_data as usual\n",
    "        if count_per_dir_dict[path] <= max_count / len(dirs):\n",
    "            X_data.extend(X_data_temp) \n",
    "        else: \n",
    "            count_for_dir = int(np.ceil(count_remaining/num_dirs_in_excess))\n",
    "            random_indices = np.random.choice(len(X_data_temp), size=count_for_dir, replace=False)\n",
    "            X_data_temp = [X_data_temp[i] for i in random_indices]\n",
    "            X_data.extend(X_data_temp) \n",
    "    y_data = [cat_num for i in range(len(X_data))]\n",
    "    X_data, y_data = process_data(X_data, y_data)\n",
    "    return X_data, y_data\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self):\n",
    "        self.X_data = []\n",
    "        self.y_data = []\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.X_data, self.y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "op\n",
      "cp\n",
      "l_\n",
      "fi\n",
      "tu\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# define gesture input directories and make gesture dictionary\n",
    "op_input_dirs = ['2021_01_29_T19_16_17_open_palm_without_glove//pure_data']\n",
    "cp_input_dirs = ['2021_02_07_T22_01_32_closed_palm//good_data']\n",
    "l_input_dirs = ['2021_02_07_T22_02_36_L//good_data']\n",
    "fi_input_dirs = ['2021_02_01_T14_28_23_fist//pure_data']\n",
    "tu_input_dirs = ['2021_02_07_T22_03_02_thumbs_up//good_data//front', '2021_02_07_T22_03_02_thumbs_up//good_data//side']\n",
    "\n",
    "# number, name, directories\n",
    "gestures_dict = {\n",
    "    'op': [0, 'Open palm', op_input_dirs],\n",
    "    'cp': [1, 'Closed palm', cp_input_dirs],\n",
    "    'l_': [2, 'L', l_input_dirs],\n",
    "    'fi': [3, 'Fist', fi_input_dirs],\n",
    "    'tu': [4, 'Thumbs up', tu_input_dirs],\n",
    "    }\n",
    "\n",
    "# Concatenate image datasets\n",
    "max_count = 500    # number of images per category\n",
    "# X_data_prev = np.zeros((max_count, 144, 256, 3))\n",
    "# y_data_prev = np.zeros((max_count, 1))\n",
    "try:\n",
    "    del X_data, y_data, X_data_prev, y_data_prev\n",
    "except:\n",
    "    pass\n",
    "X_data = []\n",
    "y_data = []\n",
    "for key in list(gestures_dict.keys()):\n",
    "    print(key)\n",
    "    # Process the data\n",
    "    cat_num = gestures_dict[key][0]\n",
    "    name = gestures_dict[key][1]\n",
    "    dirs = gestures_dict[key][2]\n",
    "    if dirs != []:\n",
    "        X_data_temp, y_data_temp = walk_file_tree(dirs, cat_num, max_count)\n",
    "        # concatenate data\n",
    "        try:\n",
    "            X_data = np.concatenate((X_data_temp, X_data), axis=0)\n",
    "            y_data = np.concatenate((y_data_temp, y_data), axis=0)\n",
    "        except:\n",
    "            X_data = X_data_temp\n",
    "            y_data = y_data_temp\n",
    "    \n",
    "y_data = to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape: (2500, 144, 256, 3)\n",
      "y_data shape: (2500, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_data shape: {X_data.shape}')\n",
    "print(f'y_data shape: {y_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d29dab2700>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "orig = X_data[0]\n",
    "save_to_dir = 'C://Users//clemo//git//motion_identification//model_preparation//augmented_images'\n",
    "\n",
    "# save original\n",
    "orig_path = os.path.join(save_to_dir, 'original.jpg')\n",
    "plt.imsave(orig_path, orig)\n",
    "\n",
    "# flip image 180 degrees\n",
    "img = tf.image.rot90(orig)\n",
    "img = tf.image.rot90(img)\n",
    "flipped_path = os.path.join(save_to_dir, 'flipped.jpg')\n",
    "tf.keras.preprocessing.image.save_img(flipped_path, img)\n",
    "\n",
    "# flip image horizontal and flip resultant image 180 degrees \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "x = orig.reshape((1,) + orig.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "for img in datagen.flow(x, batch_size=1):\n",
    "    # mirror image\n",
    "    img = img.reshape((144,256,3))\n",
    "    mirrored_path = os.path.join(save_to_dir, 'mirrored.jpg')\n",
    "    plt.imsave(mirrored_path, img)\n",
    "   \n",
    "    # flip mirred image 180 degrees\n",
    "    img = tf.reshape(img, (144, 256, 3))\n",
    "    img = tf.image.rot90(img)\n",
    "    img = tf.image.rot90(img)\n",
    "    mirrored_flipped_path = os.path.join(save_to_dir, 'mirrored_flipped.jpg')\n",
    "    tf.keras.preprocessing.image.save_img(mirrored_flipped_path, img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 256, 3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = plt.imread(orig_path)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=45,\n",
    "        width_shift_range=0.2, # horizontal shift\n",
    "        height_shift_range=0.2, # vertical shift\n",
    "        zoom_range=0.2, # zoom\n",
    "        horizontal_flip=True, # horizontal flip\n",
    "        brightness_range=[0.2,1.2]) # brightness\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train_rgb)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(X_train_rgb, y_train_rgb, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train_rgb) / 128, epochs=10, validation_data=(X_test_rgb, y_test_rgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=12, stratify=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'saved_model.hdf5'\n",
    "# path = join(par_dir, input_dir)\n",
    "model_checkpoint = ModelCheckpoint(filepath=input_dir, save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                               min_delta=0,\n",
    "                               patience=10,\n",
    "                               verbose=1,\n",
    "                               mode='auto',\n",
    "                               restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(height, width):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(height, width, 3)) # topless model\n",
    "\n",
    "    # Add top\n",
    "    base = base_model.output\n",
    "    flat = Flatten()(base)\n",
    "    fc1 = Dense(128, activation='relu', name='fc1')(flat)\n",
    "    fc2 = Dense(128, activation='relu', name='fc2')(fc1)\n",
    "    fc3 = Dense(128, activation='relu', name='fc3')(fc2)\n",
    "    drop = Dropout(0.5)(fc3)\n",
    "    fc4 = Dense(64, activation='relu', name='fc4')(drop)\n",
    "    out = Dense(num_categories, activation='softmax')(fc4)\n",
    "    model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "    # Train top layers only\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    return model\n",
    "\n",
    "height = X_train.shape[1]\n",
    "width = X_train.shape[2]\n",
    "num_categories = y_train.shape[1]\n",
    "try: \n",
    "    assert X_train.shape[3] == 3\n",
    "except AssertionError:\n",
    "    print(f'Training data has {X_train.shape[3]} color layers.')\n",
    "        \n",
    "model = create_model(height, width)\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 210s 7s/step - loss: 1.5022 - accuracy: 0.3455 - val_loss: 0.4826 - val_accuracy: 0.8150\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 228s 8s/step - loss: 0.4029 - accuracy: 0.8518 - val_loss: 0.0896 - val_accuracy: 0.9750\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 226s 8s/step - loss: 0.0942 - accuracy: 0.9723 - val_loss: 0.0794 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d29db0cb50>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.1, verbose=1, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'C://Users//clemo//git//motion_identification//motion_identification//models'\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y-%m-%d_T%H_%M\")\n",
    "model_name = date + '_VGG_model.h5'\n",
    "import os\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save final model and test import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'C://Users//clemo//git//motion_identification//motion_identification//models'\n",
    "model_paths = []\n",
    "for directory, subdirectories, files in os.walk(model_dir):\n",
    "    for file in files:\n",
    "        if file[-2:] == 'h5':\n",
    "            model_path = os.path.join(model_dir, file)\n",
    "            model_paths.append(model_path)\n",
    "model_paths\n",
    "from keras.models import load_model\n",
    "model2 = load_model(model_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def get_classification_metrics(X_val, y_val):\n",
    "    pred = model.predict(X_val)\n",
    "#     pdb.set_trace()\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "    print(classification_report(y_true, pred, target_names=['hey', 'you', 'there', 'this', 'is']))\n",
    "    test = pd.DataFrame(confusion_matrix(y_true, pred), \n",
    "             index=['hey', 'you', 'there', 'this', 'is'], columns=['hey', 'you', 'there', 'this', 'is'])\n",
    "    print(test)\n",
    "    print(f1_score(y_true, pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10170583, 0.03374023, 0.19882494, 0.53836167, 0.1273673 ],\n",
       "       [0.09943277, 0.03823787, 0.20434614, 0.51653033, 0.14145283],\n",
       "       [0.08349452, 0.02642032, 0.17068663, 0.603272  , 0.11612646],\n",
       "       ...,\n",
       "       [0.09820886, 0.03500669, 0.20160364, 0.5267183 , 0.13846244],\n",
       "       [0.07855569, 0.02161136, 0.14509979, 0.659472  , 0.09526113],\n",
       "       [0.09034568, 0.03597779, 0.2041334 , 0.5233283 , 0.14621484]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.argmax(y_val, axis=1)\n",
    "pred = model.predict(X_val)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VGG_cross_validated model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         hey       0.00      0.00      0.00         9\n",
      "         you       0.00      0.00      0.00        10\n",
      "       there       0.00      0.00      0.00         7\n",
      "        this       0.24      1.00      0.39        12\n",
      "          is       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.24        50\n",
      "   macro avg       0.05      0.20      0.08        50\n",
      "weighted avg       0.06      0.24      0.09        50\n",
      "\n",
      "       hey  you  there  this  is\n",
      "hey      0    0      0     9   0\n",
      "you      0    0      0    10   0\n",
      "there    0    0      0     7   0\n",
      "this     0    0      0    12   0\n",
      "is       0    0      0    12   0\n",
      "0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\clemo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\clemo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\clemo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "get_classification_metrics(X_val[:50], y_val[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict gesture on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C://Users//clemo//git//motion_identification//model_preparation//2021_02_01_T14_28_23_fist//pure_data//218.jpg'\n",
    "okay_open_palm = 'C://Users//clemo//git//motion_identification//model_preparation//2021_01_29_T19_16_17_open_palm_without_glove//poor_data//176.jpg'\n",
    "tricky_open_palm = 'C://Users//clemo//git//motion_identification//model_preparation//2021_01_29_T19_16_17_open_palm_without_glove//poor_data//397.jpg'\n",
    "bad_open_palm = 'C://Users//clemo//git//motion_identification//model_preparation//2021_01_29_T19_16_17_open_palm_without_glove//poor_data//1606.jpg'\n",
    "bad_open_palm2 = 'C://Users//clemo//git//motion_identification//model_preparation//2021_01_29_T19_16_17_open_palm_without_glove//poor_data//112.jpg'\n",
    "tricky_open_palm2 = 'C://Users//clemo//git//motion_identification//model_preparation//2021_01_29_T19_16_17_open_palm_without_glove//poor_data//695.jpg'\n",
    "tricky_open_palm3 = 'C://Users//clemo//git//motion_identification//model_preparation//2021_01_29_T19_16_17_open_palm_without_glove//poor_data//1968.jpg'\n",
    "\n",
    "okay_fist = 'C://Users//clemo//git//motion_identification//model_preparation//2021_02_01_T14_28_23_fist//poor_data//535.jpg'\n",
    "tricky_open_palm = 'C://Users//clemo//git//motion_identification//model_preparation//2021_02_01_T14_28_23_fist//poor_data//553.jpg'\n",
    "bad_fist = 'C://Users//clemo//git//motion_identification//model_preparation//2021_02_01_T14_28_23_fist//good_data//1840.jpg'\n",
    "\n",
    "paths = []\n",
    "paths.append(okay_open_palm)\n",
    "paths.append(tricky_open_palm)\n",
    "paths.append(bad_open_palm)\n",
    "paths.append(bad_open_palm2)\n",
    "paths.append(okay_fist)\n",
    "paths.append(tricky_open_palm)\n",
    "paths.append(bad_fist)\n",
    "paths.append(tricky_open_palm2)\n",
    "paths.append(tricky_open_palm3)\n",
    "\n",
    "def predict_image(path):\n",
    "    img = Image.open(path)\n",
    "    img = np.array(img)\n",
    "    img = np.array(img, dtype = 'float32')\n",
    "    img = np.stack((img,)*3, axis=-1) # without comma, (X_data) is np.array not tuple\n",
    "    pdb.set_trace()\n",
    "#     img /= 255\n",
    "    img.resize((1, 144, 256, 3))\n",
    "    plt.figure()\n",
    "    plt.imshow(img[0])\n",
    "    return np.argmax(model2.predict(img))\n",
    "\n",
    "def load_image(path):\n",
    "    img = Image.open(path)\n",
    "    img = np.array(img)\n",
    "#     img = np.stack((img,)*3, axis=-1) # without comma, (X_data) is np.array not tuple\n",
    "#     img /= 255\n",
    "#     plt.figure()\n",
    "#     plt.imshow(img[0])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for path in paths:\n",
    "    result = predict_image(path)\n",
    "    results.append(result)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=45,\n",
    "        width_shift_range=0.2, # horizontal shift\n",
    "        height_shift_range=0.2, # vertical shift\n",
    "        zoom_range=0.2, # zoom\n",
    "        horizontal_flip=True, # horizontal flip\n",
    "        brightness_range=[0.2,1.2]) # brightness\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train_rgb)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(X_train_rgb, y_train_rgb, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train_rgb) / 128, epochs=10, validation_data=(X_test_rgb, y_test_rgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
